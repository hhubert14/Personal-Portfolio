---
title: "Decision Tree"
category: "Intro to Machine Learning (CSE417T)"
date: "11/27/2024"
---
# Decision Tree
Most decision tree learning algorithms follows this template with with a different choices of heuristics:
DecisionTreeLearn(ğ·): Input a dataset ğ·, output a decision tree hypothesis
    Create a root node
    If termination conditions are met
        return a single node tree with leaf prediction based on ğ·
    Else: Greedily find a feature ğ´ (assigned as root) to split according to split criteria
        For each possible value ğ‘£ of ğ´
            Let ğ·_i be the dataset containing data with value ğ‘£_i for feature ğ´
            Create a subtree DecisionTreeLearn(ğ·) that being the child of root
TODO understand this template better

## Split criteria
One split criteria is the Iterative Dichotomiser 3 (ID3) algorithm:
TODO: discuss information entropy adn information gain to show algo

## To Address Overfitting
 â€¢ More Regularization (Constrain ğ»)
    â€¢ Do not split leaves past a fixed depth
    â€¢ Do not split leaves with fewer than ğ‘ labels
    â€¢ Do not split leaves where the maximal information gain is less than ğœ
 â€¢ Pruning (removing leaves)
    â€¢ Evaluate each split using a validation set and compare the validation error with and 
    without that split (replacing it with the most common label at that point)
    â€¢ Use statistical test to examine whether the split is â€œinformativeâ€ (leads to different enough subtrees)